%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                         CHAPITRE 2                            %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\lhead[\fancyplain{}{\leftmark}]%Pour les pages paires \bfseries
      {\fancyplain{}{}} %Pour les pages impaires
\chead[\fancyplain{}{}]%
      {\fancyplain{}{}}
\rhead[\fancyplain{}{}]%Pour les pages paires 
      {\fancyplain{}{\rightmark}}%Pour les pages impaires \bfseries
\lfoot[\fancyplain{}{}]%
      {\fancyplain{}{}}
\cfoot[\fancyplain{}{\thepage}]%\bfseries
      {\fancyplain{}{\thepage}} %\bfseries
\rfoot[\fancyplain{}{}]%
     {\fancyplain{}{\scriptsize}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                      Start part here                          %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Theoretical framework}
\label{ch:2}

%==============================================================================	Résumé du chapitre

\begin{center}
\rule{0.7\linewidth}{.5pt}
\begin{minipage}{0.7\linewidth}
\smallskip

\textit{Obtaining kinematics from a network of calibrated video cameras means resolving a few theoretical points. First, features must be recognized in images. This is now mostly done with machine learning models. Then, cameras need to be calibrated, so that all of the 2D features detected for each cameras can be reconstructed in the 3D space. Finally, these coordinates must be constrained to a biomechanically consistent model, in order to obtain coherent joint kinematics.}

%\smallskip
\end{minipage}
\smallskip
\rule{0.7\linewidth}{.5pt}
\end{center}

\minitoc
\newpage


\section{Pose detection}

\subsection{Feature detection}

As a first step, achieving motion analysis from a network of cameras involves detecting features in images. These features can be whole human beings, joint centers, body landmarks, sports gear such as tennis balls, climbing holds, or much more. 

Two broad approaches can be implemented: the first one consists in using dedicated algorithms for each task. The gist of it is to understand the task well enough to build an appropriate solution. Among other techniques, corner and contour detection, color thresholding, affine transformation, template matching, watershed segmentation, can be used. For example, if one wants to differentiate two boxers wearing respectively a blue and a red shirt, they can filter them by color. If one needs to identify on which portion of a speed climbing wall an athlete is, they can match the template of each holds on the whole image. OpenCV \cite{Bradski2000} provides convenient tools for this purpose, in C++ and Python languages. This approach is often fast, but also quite complicated to implement, and neither flexible nor robust. If there is other red or blue patches in the boxing scene, if the boxer wears green or if the light is poor, this will not work anymore. Likewise for holds, if the sun casts a large shadow which changes its apparent shape, or if holds are seen from a different perspective.

The second approach takes advantage of machine learning algorithms, which constitute an entirely different paradigm. The idea is to show the machine enough examples for it to "understand" by itself its underlying attributes, so that it manages to detect and label automatically new images. This can be used for both aforementioned tasks, in a much more flexible way: if one wants the system to recognize boxers or holds in challenging condition, they simply have to include such examples while training the machine learning model. The machine learning approach is also suitable for other tasks, such as whole-image classification (i.e., determining whether this a boxing or a climbing scene), background extraction \cite{Bouwmans2019}, instance segmentation (i.e., extracting the shape of the climber, as well as each holds, the wall, the background, etc.) \cite{Minaee2021}, or keypoint detection (e.g., localization of human joint centers in an image \cite{Chen2020}). 


\subsection{Machine learning timeline and principles}

Machine learning is a subset of artificial intelligence (AI.) As such, one can trace its origin back to the discovery of the natural neuron at the end of the 19th century, by Nobel Prize Ramón y Cajal \cite{Lopez2006}, followed half a century later by the first model of an artificial neuron \cite{Mcculloch1943}. A natural neuron is a simple learning unit, which collects the nervous influx sent by other neurons to its dendrites, and sends an action potential when the total influx weighted and summed in the soma overcomes a threshold value. This potential is then transmitted through the axon to the next neuron as a new influx. Similarly, an artificial neuron receives the output vector from previous neurons, weighs and sums them with a summation function, and transfers the resulting output vector to the next neurons if it reaches a certain threshold (Figures~\ref{fig_neuron}). 

\begin{figure}[hbtp]
	\centering
	\def\svgwidth{1\columnwidth}
	\fontsize{10pt}{10pt}\selectfont
	\includegraphics[width=\linewidth]{"../Chap2/Figures/Fig_neuron.png"}
	\caption{The artificial neuron has been modeled after the natural neuron. Inputs and weights act as the total of nervous influx firing the dendrites. The collected values are summed, and a signal is activated if a threshold is overcome, as the soma does in a natural neuron. The output signal is conveyed through the axon.}
	\label{fig_neuron}
\end{figure}

The perceptron, invented in 1956 \cite{Rosenblatt1958}, represents the first practical application of an artificial neuron. This is a binary classifier updating automatically its weights from data. 



Given a training set, it could learn
It could be used, for example, to classify whether an athlete is strong or not, given his test results 

The perceptron rule is 


\begin{equation} 
      h = w_0 + \overrightarrow{w} \cdot \overrightarrow{q} 
\end{equation}
      
\begin{equation} 
      \overrightarrow{w^{t_1}} = \overrightarrow{w^{t_0}} + r\, c\, \overrightarrow{q^{t_0}} 
\end{equation}
      
\begin{multline}
      \biggl\{(\overrightarrow{q_i}, \overrightarrow{c_i})\biggr\}_{i\in [1,n]} \\
      = \Biggl\{ 
      \biggl( \begin{pmatrix} 1 \\ 5 \end{pmatrix}, 1 \biggr),
      \biggl( \begin{pmatrix} 2 \\ 3 \end{pmatrix}, -1 \biggr),
      \biggl( \begin{pmatrix} 4 \\ 1 \end{pmatrix}, -1 \biggr),
      \biggl( \begin{pmatrix} 5 \\ 4 \end{pmatrix}, 1 \biggr),
      \biggl( \begin{pmatrix} 8 \\ 1 \end{pmatrix}, 1 \biggr)
      \Biggr\}
\end{multline}
      
      
in hN and in m/s
      
\begin{equation}
      h(\overrightarrow{q}) = 
      \begin{cases}
      1 & \text{if} \ w_0 + \overrightarrow{w} \cdot \overrightarrow{q} > 0,\\
      0 & \text{otherwise}
      \end{cases}
\end{equation}


w0 biais

Limitation


Possible de jouer sur le learning rate, sur les poids initiaux, ainsi que sur la fonction d'activation (exemple: sigmoïde)

Pas possible si non linéairement séparable (exemple image)

Define learning step $\eta$ 

Winters 1966 and 1988 (XOR, Turing, power, funding)
"the spirit is willing but the flesh is weak." Translated back and forth with Russian, it became "the vodka is good but the meat is rotten." during cold war. Dictionary without context

CNN

Deep learning

-> perceptron mono/multicoucheNeurone: unité d'apprentissage



History natural neuron and formal neuron (dates, names, comparison)

Timeline cahier jaune et wiki

S'inspirer de wikipedia (en, fr, timeline); 
S'inspirer du mail machine learning starred (exemple du réseau de neurones); 
S'inspirer du cahier jaune


- deep learning vs CNN (vs SVM, random forest, etc.), AI, machine learning

- classification vs detection vs segmentation

data augmentation, dropout, batch normalization
overfitting
train / test
accuracy loss
gradient descent
layers, batch size, epochs, activation

transfer learning

% \subsection{Application to object detection and localization}


\subsection{Pose detection}

Different architectures, different models, different datasets


\section{3D reconstruction}\label{sec:3D reconstruction}

While some approaches only rely on 2D pose estimation to infer 3D pose with another machine learning model, they are generally not considered to be sufficiently reliable. It is, then, important to use the input from several cameras, and to fuse their informations to obtain 3D coordinates.


\subsection{Pinhole camera model}

Voilà


\subsection{Calibration}

test


\subsection{Triangulation}

suite


\section{3D joint kinematics}

\subsection{Physically consistent model}

autre


\subsection{Scaling}

bref


\subsection{Inverse kinematics}

As opposed to forward kinematics \newline
Compare with 2D angles between 3 points \newline
Different methods (model based vs autres) for angles (cf mail starred)\newline




